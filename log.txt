2015-12-11 14:03:15
弄到了shadowsocks的源码(python的)，打算爬一爬，然后尝试自己造个轮子出来
途中可能会遇到各种相关问题、技术、idiom
比如 absolute_import, select/poll..
都在这里实践实践

2015-12-11 14:06:50
先来 absolute_import
    
    # shadowsocks/local.py
    from __future__ import absolute_import

https://www.python.org/dev/peps/pep-0328/

2015-12-11 17:15:08
TODO
https://en.wikipedia.org/wiki/Epoll
https://en.wikipedia.org/wiki/Kqueue
https://en.wikipedia.org/wiki/Overlapped_I/O
测试测试这些不同的异步IO模型的性能

2015-12-11 17:45:27
看完eventloop.py差不多理解事件驱动部分了
就是拿 select 来异步处理所有 sockets 的读写

2015-12-11 20:27:25
tcp server socket 在
    lsock.bind(addr)
    lsock.listen(1024)
之后，
    select.select([lsock], [], [])
一旦 readable ，那么一定就是
    lsock.accept()

2015-12-11 21:03:59
https://github.com/shadowsocks/shadowsocks/tree/master
https://en.wikipedia.org/wiki/SOCKS
clowwindy的python这版的shadowsocks client直接不看来的TCP连接greeting是什么内容，sock.recv(32KB) 之后直接返回 '\x05\x00' —— 意即使用 SOCKS5 协议，no authentication

https://shadowsocks.org/en/download/clients.html
然后我测试了下 shadowsocks.org 的 windows client
发现 greeting 发两个字节(第一个必须是'\x05'，第二个无所谓)就会收到回复 '\x05\x00'

2015-12-11 21:16:05
发现给 win client 用 TCP socket 发 HTTP 请求的时候，正常的
    GET / HTTP/1.1
    Host: baidu.com
不好使，会被返回
    HTTP/1.1 400 Invalid header received from client
去chrome实地考察(设置 http proxy 为 127.0.0.1:1080)，发现人家发的是
    GET http://www.baidu.com/ HTTP/1.1

好像想起来了复试完在家当时宋美娜让看 HTTP The Definitive Guide 里面有提到
发给 proxy 的请求跟发给正常 server 的请求有所区别
等我去查查

https://en.wikipedia.org/wiki/Proxy_server
Proxy Server —— 先爬这家伙

2015-12-11 22:08:11
https://www.ietf.org/rfc/rfc2616.txt
gotcha —— 这里说“The absoluteURI form is REQUIRED when the request is being made to a proxy”
至于为什么..
http://stackoverflow.com/questions/6586882/why-do-http-proxies-require-an-absolute-uri-in-their-get-requests
这里说 Host header 是 HTTP1.1 才有的，那么在1.0的时候，只有 GET <uri>
这对正常服务器是ok的，因为服务器知道要取的资源(/foo/bar.txt)是在自己这里
但如果一个proxy收到 GET /foo/bar.txt HTTP/1.0 ，它就懵逼了~~
所以用 absoluteURI 的办法解决
后来到1.1有了Host，但之前的解决方式还是存留了下来

HTTP The Definitive Guide 里面讲的很清楚，确实是1.0的遗留问题

2015-12-11 22:24:46
那么目前从代码来看，clowwindy 的 client 只支持 SOCKS5 连接
ss.org 的 win client 支持 http, https, ftp, socks4, socks5 连接

其实还有一个疑问，就是TCP往上是不区分应用层协议的
win client 在 1080 端口接收所有的 tcp 数据，它是怎么区分来流是 http 呢还是 socks 还是别的什么？通过内容分析吗？

2015-12-11 22:47:44
https://github.com/shadowsocks/shadowsocks-windows/blob/master/shadowsocks-csharp/Controller/Service/TCPRelay.cs
哈，原来 win client 也是 clowwindy 写的——用的 C#
这里一句
    if (length < 2 || firstPacket[0] != 5)
    {
        return false;
    }
正应了我前面描述的行为~~~

2015-12-11 22:57:13
https://github.com/shadowsocks/shadowsocks-windows/search?q=http&type=Issues&utf8=%E2%9C%93
刚想起来之前用ss经常碰到一个叫prioxy的家伙
在shadowsocks-windows的issues里看提到这个，加之在ss-win的代码里找不到http proxy相关——
忽然意识到，ss是开了个 prioxy (这家伙本身就是个HTTP代理软件)然后自己作为二级代理！
咦，但是它们是怎么共用的1080端口？

2015-12-12 00:18:15
快要触及多协议共用端口的秘密了
Shadowsocks.exe 会启动一个 ss_privoxy.exe
实验了下杀掉ss_privoxy，然后python写socket通过TCP发HTTP请求，ss就懵逼了(没有回复)，但此时发SOCKS5 greeting还是有回复的
可以想见ss是把非SOCKS协议的处理都delegate给ss_privoxy了
但是如何识别进来的TCP流是非SOCKS协议呢，继续追踪代码(PolipoRunner)

2015-12-12 00:30:15
https://github.com/shadowsocks/shadowsocks-windows/blob/2.5.8/shadowsocks-csharp/Controller/ShadowsocksController.cs
https://github.com/shadowsocks/shadowsocks-windows/blob/2.5.8/shadowsocks-csharp/Controller/Service/PortForwarder.cs
感觉重点在这两个文件里，但是有点晚了，还没看明白，，明儿继续

2015-12-12 10:28:57
感觉是明白了——在厕所时候琢磨到可能是ss-client收到的所有数据都转一份给privoxy，那么privoxy识别出来的http,https,ftp的流量就转成socks5再发回给ss-client

回来看代码，重点在这几处

    // shadowsocks-csharp/Controller/ShadowsocksController.cs
    List<Listener.Service> services = new List<Listener.Service>();
    services.Add(tcpRelay);
    services.Add(udpRelay);
    services.Add(_pacServer);
    services.Add(new PortForwarder(polipoRunner.RunningPort));
    _listener = new Listener(services);
    _listener.Start(_config);

    // shadowsocks-csharp/Controller/Service/Listener.cs
    foreach (Service service in _services)
    {
        if (service.Handle(buf, bytesRead, conn, null))
        {
            return;
        }
    }

也就是说，tcpRelay, udpRelay, PortForwarder 这几个家伙是依次处理来流的

    // shadowsocks-csharp/Controller/Service/TCPRelay.cs
    if (socket.ProtocolType != ProtocolType.Tcp)
    {
        return false;
    }
    if (length < 2 || firstPacket[0] != 5)
    {
        return false;
    }

这里TCPRelay就把非 SS5 的来流略过了

    // shadowsocks-csharp/Controller/Service/UDPRelay.cs
    if (socket.ProtocolType != ProtocolType.Udp)
    {
        return false;
    }

UDPRelay 略过非UDP来流

然后就到了 PortForwarder 这里

    // shadowsocks-csharp/Controller/Service/PortForwarder.cs
    if (socket.ProtocolType != ProtocolType.Tcp)
    {
        return false;
    }
    new Handler().Start(firstPacket, length, socket, this._targetPort);
    return true;

Handler 做的基本就是原样转来流给 privoxy 了
这里的 `this._targetPort` 是这么来的：
    
    // shadowsocks-csharp/Controller/ShadowsocksController.cs
    services.Add(new PortForwarder(polipoRunner.RunningPort));

然后 polipoRunner 做的就是启动 privoxy

至此，真相大白~ 就是：
ss-client 收到 ss5 来流就直接处理，非ss5的转给 privoxy
然后 privoxy 处理过再转回给 ss-client (以类似chrome这样的正常application的身份)
哦，对，在 ss_privoxy.exe 所处的文件夹里还有个 privoxy.conf ，内容如下

    # C:\Users\Administrator\AppData\Local\Temp\privoxy.conf
    listen-address 0.0.0.0:8123
    show-on-task-bar 0
    activity-animation 0
    forward-socks5 / 127.0.0.1:1080 .
    hide-console

关于 privoxy 的整个流程自己假想了成立的但还没去看代码验证
另外UDP DNS部分还没研究
但起码之前疑惑的地方已经都明白了

那么下来如果自己要写 fadowsocks，需要搞懂的就是 SOCKS5 协议了
而且既然 ss-client 直接用 no authentication 的方式，自己也可以这么简单化处理
client 跟 server 之间的协议就完全无所谓了，哪怕设计成 ROT13 都可以(但是加密总是应该的，怕被GFW内容分析)
然后server那里就是正常的TCP了
DNS部分也需要搞懂
但这些就都是之后的事儿了，，手头这茬已圆满结束

2015-12-13 11:01:12
可以自己着手写了，情景不是很清楚，不过可以确定第一个阶段性目标：
在本地和VPS上部署两个进程，使得可以通过chrome的SOCKS5代理协议访问google.com

那么起手要做的，还是爬懂SOCKS5的wiki

2015-12-13 11:19:02
貌似chrome只会发 no authentication 的 socks5 greeting，所以 local client 是必要的

chrome给的是domain name，那么到vps那边就得做DNS，不过python的sock.connect直接可以用domain name做地址，所以可以现在就试试

2015-12-13 13:41:48
呐，实现了
    HEAD www.google.com HTTP/1.1
本地一个 local.py ，VPS 上一个 server.py
local 监听 6560 接收 SOCKS5 请求，转流给 server (rot13编码)
server 建立 SOCKS header 里指定的 domain_name port 连接，拿到结果后回传给 client

因为写得太原型，还没敢直接用chrome试，自己t.py手写的请求
等我 select 并发搞起，错误控制做好，再来试浏览器的

不知道今儿半天能搞定不？

2015-12-13 14:33:03
我靠，一切换到 select 的模型忽然好难写

2015-12-14 17:10:18
client 收到 chrome 的 cmd request ("establish a tcp connection to google.com") 后，需要建立与 server 的 tcp 连接
    sock.connect(..)
直接在 on_recv 里头执行这个家伙会 defeat the whole point of using `select`
因为这家伙实际上是 blocking 的
于是再次引出 TCP Handshake，，在本机和Ubuntu VM上跑起python的 server, client然后 wireshark 抓包，现在在研究 tcp 的 psh flag

2015-12-14 18:28:11
http://packetlife.net/blog/2011/mar/2/tcp-flags-psh-and-urg/

http://stackoverflow.com/questions/2264154/when-is-the-push-flag-set-in-tcp-segment
TCP_NODELAY - 这个也在shadowsocks的源码里见过
MSG_MORE - 这个是不是 PSH 置零？

https://en.wikipedia.org/wiki/Nagle%27s_algorithm
Nagle's algorithm 就是通过buffer application layer交下来的数据来提升网络利用率，但是会造成某些实施应用的高latency
想试试能不能用socket测试出这种效应

2015-12-14 20:17:52
http://stackoverflow.com/questions/2231283/tcp-two-sides-trying-to-connect-simultaneously
这里说TCP可以两方同时connect——即同时SYN, SYN/ACK, ACK
这样也可以建立连接

2015-12-14 20:58:57
可以用 TCP_NODELAY 来 disable Nagle algorithm
    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY)
    for _ in range(5):
        sock.send('a')

不设置的时候wireshark捕捉到有数据的包是2个
    a (PSH, ACK, Len=1)
    aaaa (PSH, ACK, Len=4)
设置之后是5个
    a (PSH, ACK, Len=1)
    a (PSH, ACK, Len=1)
    a (PSH, ACK, Len=1)
    a (PSH, ACK, Len=1)
    a (PSH, ACK, Len=1)

2015-12-14 21:06:26
http://stackoverflow.com/questions/9153566/difference-between-push-and-urgent-flags-in-tcp
PUSH 和 URG 不太一样

2015-12-14 21:16:15
发现 send('a' * 1460 * 2) 的时候，wireshark 就会检测到两个包
一个 [ACK, Len=1460], 紧跟着一个 [PSH, ACK, Len=1460]

不是很清楚地感觉：TCP_NODELAY 只是在帮助处理 small packet 跟 Nagle's algorithm 的关系


忽然想起来为什么5个a中第一个会 PSH 而后面四个被 aggregate 到一个 packet 里了
https://en.wikipedia.org/wiki/Nagle%27s_algorithm
Nagle's algorithm描述里:

    if there is new data to send
      if the window size >= MSS and available data is >= MSS
        send complete MSS segment now
      else
        if there is unconfirmed data still in the pipe
          enqueue data in the buffer until an acknowledge is received
        else
          send data immediately

重点在“如果有未ACK的数据仍在pipe中，就buffer application的数据”
发第一个a时执行的是 `send data immediately` 这句
后面的4个a走了 `if there is until data still...` 于是被aggregate起来

https://en.wikipedia.org/wiki/Transmission_Control_Protocol
https://en.wikipedia.org/wiki/IPv4#Header
https://en.wikipedia.org/wiki/Ethernet_frame
顺便说一下1460的来历：
Ethernet Frame maximum length 是 1518 (如果有VLAN tag则是1522)
payload 最大 1500(因为Ethernet Header + Checksum 总共是18字节; VLAN 22字节)
然后 1500 - 20(ip) - 20(tcp) == 1460
如果是 UDP 的话 1500 - 20(ip) - 8(udp) == 1472

http://serverfault.com/questions/422158/what-is-the-in-the-wire-size-of-a-ethernet-frame-1518-or-1542
这里讲得挺清楚

2015-12-14 21:55:32
所以有关1460的故事主要是涉及“用户的data stream是怎么变成一个个TCP Segment的”
当用户给的数据很大时，比如3000B
os会给分成三段
    [ACK] Len=1460
    [ACK] Len=1460
    [PSH, ACK] Len=80
当数据很小时，比如5个'a'，Nagle会aggregate成2个Segment
    [PSH, ACK] Len=1
    [PSH, ACK] Len=4
当设置了TCP_NODELAY时，5个Segment
    [PSH, ACK] Len=1
    [PSH, ACK] Len=1
    [PSH, ACK] Len=1
    [PSH, ACK] Len=1
    [PSH, ACK] Len=1
(貌似PSH flag还会影响到peer的delayed acknoledgement？)

http://serverfault.com/questions/614590/what-is-psh-ack-doing-during-my-connection-to-a-global-catalog-server
还有个TCP_QUICKACK的选项

http://blog.mafr.de/2010/03/14/tcp-for-low-latency-applications/
force a socket to send the data in its buffer
Nagle's algorithm
TCP_NODELAY
这三者的关系还是不很清楚

http://www.unixguide.net/network/socketfaq/2.11.shtml


TCP_NODELAY与PSH flag没有必然关系，比如 sock.send('a' * 3000) 的时候
前两个 1460 的 segment 还是 [ACK]，只有最后一个 80 的 segment 是 [PSH, ACK]

2015-12-14 23:45:22
http://edsiper.linuxchile.cl/blog/2013/02/21/linux-tcp-fastopen-in-your-sockets/
shadowsocks 进行 async connect 的做法是 TCP FASTOPEN，用的是 tcpsock.sendto

不使用fastopen的话可以
    sock.setblocking(0)
    sock.connect_ex(addr)
    select.select(..)
需要使用 connect_ex ，这样才不会抛出
    [10035] A non-blocking socket operation could not be completed immediately
的Exception

2015-12-15 12:16:19
发现 ubuntu 的MTU是1448——比windows的1460少了12字节
哪来的？
仔细看wireshark的结果显示多了个TCP Option:
2字节的 NOP 用作padding, 然后是一个 10字节的 Timestamps

https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_timestamps
wiki提到 PAWS - Protection Against Wrapped Sequence
好像在 man tcp 里也看到过

2015-12-15 12:25:27
http://baus.net/on-tcp_cork/
TCP_CORK 的作用好像是让 TCP 只在 buffer 足够 segment 的时候发送
但是实际实验显示又不完全是这样，，这些挺难设计精确控制的实验，觉得最好还是以后直接研究对应的内核源码

http://stackoverflow.com/questions/3761276/when-should-i-use-tcp-nodelay-and-when-tcp-cork
第3个答案有提到 cork 是有上限的(目前是200ms)，timeout一到不管够不够一个packet，os也会把数据发出去

http://stackoverflow.com/questions/22117205/is-there-an-equivalent-to-tcp-cork-in-winsock
这里用 TCP_NODELAY 在 windows 上实现了 TCP_CORK 的行为

2015-12-15 12:37:07
http://ithare.com/64-network-dos-and-donts-for-game-engine-developers-part-i-client-side/
本来打算仔细过这个，但今天看了点发现是偏经验的，，那么mark一下以后再看了

2015-12-15 14:43:16
没看代码，但shadowsocks-linux-client里在 TCPRelayHandler 的 __init__ 里直接用 local_sock.getpeername()
所以假定这个函数是不会 block 的：
accept() 所得的 sock 内部已经有了 peer 的信息
所以你看 python 的 accept 直接返回的是 sock, addr

缕一下shadowsocks-linux-client的逻辑：
先创建一个 lsock (listening sock)，加到 r_list 里
当有连接到来时，accept 得到 sock (connection sock)，同样加到 r_list 里
现在是为了读 user-app 的 greeting ('\x05\x00')
...

non-blocking mode 里的 send 之前是不需要检查的(EAFP)
直接 send ，然后看发了多少，如果没发完，再去 select([], [sock], []) 以在将来继续发没发完的数据
另外，不管发没发完，都要更新 select 所操作的 r/w_list
